{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.载入数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1载入总的合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from signal import signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from preprocessSpectral import preprocessSpectral\n",
    "from  matplotlib import pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import sys\n",
    "import zipfile\n",
    "# 标记字典\n",
    "dfs = []\n",
    "labelDict = {'1': 1,'2':2,'3':3,'G11':4,'G12':5,'G13':6}\n",
    "\n",
    "# 读入数据\n",
    "xlsxfiles = glob.glob('hyperdata/224_531/*.xlsx')\n",
    "sd = pd.DataFrame()\n",
    "avgs = pd.DataFrame()\n",
    "for file in xlsxfiles:\n",
    "    filename = file.split('\\\\')[1]\n",
    "    label = labelDict[filename.split('_')[0]]\n",
    "    data = pd.read_excel(file,index_col = 0)\n",
    "    data=data.transpose()\n",
    "    \n",
    "\n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data = data[~np.isinf(data).any(axis=1)]\n",
    "    \n",
    "    '''选择部分行数据'''\n",
    "    # data=data.iloc[20:50,:]\n",
    "\n",
    "    '''原始的数据,没有预处理'''\n",
    "    data_process = data.to_numpy()\n",
    "    '''带预处理的数据'''\n",
    "    # data_process = preprocessSpectral.MSC(data.to_numpy())\n",
    "\n",
    "    # data_process = preprocessSpectral.SG(data_process,31,1)\n",
    "\n",
    "    # data_process = preprocessSpectral.SG(data_process,17,1)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # data_process = preprocessSpectral.D1(data_process)\n",
    "    # data_process = signal.detrend(data_process)\n",
    "    \n",
    "    print(file)\n",
    "    print(data.shape)\n",
    "    # 保存图像\n",
    "    # plt.plot(np.transpose(data_process)) \n",
    "    plt.savefig(sys.path[0]+ \"/hyperdata/data_preprocess/\" + filename.replace('xlsx', 'jpg'))\n",
    "    plt.clf()\n",
    "    # 加入数据\n",
    "    data = pd.DataFrame(data_process)\n",
    "    # 标准差\n",
    "    sd[filename.split('_')[0]] = data.std(axis=0)\n",
    "    # avgs[filename.split('.')[0]] = data.mean(axis=0)\n",
    "    data.loc[:, 'label'] = label\n",
    "    dfs.append(data) \n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2数据做PCA运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.concat(dfs,axis=0)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataspca=datas.iloc[:,:-1]\n",
    "dataspca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = datas['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=23)\n",
    "X_pca = pca.fit_transform(dataspca)\n",
    "\n",
    "# 打印每个主成分的贡献率\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=pd.DataFrame(datas)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = datas.reset_index()\n",
    "labels = labels.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalabels = pd.concat([datas,labels],axis=1)\n",
    "datalabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalabels=datalabels.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalabels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3SPA算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.concat(dfs,axis=0)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasspa=datas.iloc[:,:-1]\n",
    "datasspa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = datas['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺序1-SPA得到的原始结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasspa=datasspa.iloc[:,[138,124,  18,  54,  79, 150, 106, 169, 178, 203, 164, 130, 201,\n",
    "         6, 194, 206, 196, 200, 199, 205, 204]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasspa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalabels = pd.concat([datasspa,labels],axis=1)\n",
    "datalabels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺序2-SPA得到的结果按顺序排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datass=datasspa.iloc[:,[  6, 18,  54,  79,106,124, 130,138,  150, 164,  169, 178, 194,196,199,   200,  201,203, \n",
    "         204, 205, 206]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.concat([datass,labels],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4数据的拼接--统一跳转这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.concat(dfs,axis=0)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.to_excel(\"meizhong100.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas= datas.drop(index=23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5画图 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一类高粱画光谱图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你有一个名为df的DataFrame，包含光谱数据和标签\n",
    "\n",
    "# 提取光谱数据和标签\n",
    "spectra = datas.iloc[:, :-1]  # 光谱数据，去掉最后一列\n",
    "labels = datas.iloc[:, -1]    # 标签，最后一列\n",
    "\n",
    "# 获取不同类别的标签\n",
    "unique_labels = labels.unique()\n",
    "class_labels = ['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "# 设置绘图的颜色列表\n",
    "colors = ['#1F77B4', '#FF7F0E', '#2CA02C', '#9467BD','#E377C2','yellow']\n",
    "\n",
    "# 绘制不同类别的光谱\n",
    "for label, color in zip(class_labels, colors):\n",
    "    plt.plot([], [], color=color, label=label)  # 添加空线条，只用于图例\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    # 获取相应类别的光谱数据\n",
    "    label_spectra = spectra[labels == label]\n",
    "    \n",
    "    # 绘制光谱线条\n",
    "    plt.plot(label_spectra.T, color=color, label=label)\n",
    "\n",
    "# 添加图例\n",
    "\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一类高粱平均后画光谱图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2 = pd.DataFrame()\n",
    "fig, ax = plt.subplots()\n",
    "labels = ['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "plt.figure(figsize = (10,10))\n",
    "for i in range(0,6):\n",
    "    sd2[i+1]=datas[(datas['label']==i+1)].iloc[:,:-1].mean(axis=0)\n",
    "    ax.plot(sd2[i+1], label=labels[i])\n",
    "    # sd[filename.split('.')[0]] = data.std(axis=0)\n",
    "ax.legend()\n",
    "ax.set_xlabel('spectral')\n",
    "ax.set_ylabel('refelction')\n",
    "ax.set_title('spectral image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2.plot(title='mean')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2=sd2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['blue', 'green', 'yellow', 'orange', 'red']\n",
    "cmap = ListedColormap(colors)\n",
    "heatmap = plt.imshow(sd2, cmap='viridis', aspect='auto')\n",
    "\n",
    "# plt.colorbar()  # 添加颜色条\n",
    "labels = ['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "plt.colorbar(heatmap, cmap=cmap)\n",
    "categories =['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "plt.xticks(np.arange(len(categories)), categories)\n",
    "# colorbar = plt.colorbar(heatmap)\n",
    "# colorbar.set_label('New Label') \n",
    "plt.title('Hyperspectral Heatmap')\n",
    "plt.ylabel('spectral')\n",
    "plt.xlabel('Categories')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6组装数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovr分类使用\n",
    "labels = datas['label']\n",
    "datas = datas.drop(columns='label')\n",
    "datalabels = pd.concat([datas,labels],axis=1)\n",
    "datalabels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# one-hot编码多分类使用,在这里改标签\n",
    "labels = pd.get_dummies(datas['label'])\n",
    "classmap = {'1': 1,'2':2,'3':3,'G11':4,'G12':5,'G13':6}#,'G3':4,'G5':5,'G6':6,'G7':7,'G8':8,'G9':9,'G10':10,'G11':11,'G12':12,'G13':13,'G14':14\n",
    "labels = labels.rename(columns=classmap)\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.to_excel(\"所有光谱.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1.to_excel(\"224维光谱6.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2.to_excel(\"预处理之后的平均光谱.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2.to_excel(\"224维光谱没平均.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入测试集数据，验证实验"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: \n",
    "Version: 1.0\n",
    "Author: tangliwen\n",
    "Date: 2022-09-27 08:41:08\n",
    "LastEditors: tangliwen\n",
    "LastEditTime: 2022-10-20 14:03:27\n",
    "'''\n",
    "from signal import signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from preprocessSpectral import preprocessSpectral\n",
    "from  matplotlib import pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import sys\n",
    "import zipfile\n",
    "# 标记字典\n",
    "dfs2 = []\n",
    "labelDict = {'1': 1,'2':2,'3':3,'G11':4,'G12':5,'G13':6}\n",
    "classes = ['宜糯红4号','泸州红1号','郎糯红19号']\n",
    "# 读入数据\n",
    "xlsxfiles = glob.glob('hyperdata/验证数据2/*.xlsx')\n",
    "sd = pd.DataFrame()\n",
    "avgs = pd.DataFrame()\n",
    "for file in xlsxfiles:\n",
    "    filename = file.split('\\\\')[1]\n",
    "    label2 = labelDict[filename.split('_')[0]]\n",
    "    data2 = pd.read_excel(file,index_col = 0)\n",
    "    data2=data2.transpose()\n",
    "    \n",
    "\n",
    "    data2 = data2[~np.isnan(data2).any(axis=1)]\n",
    "    data2 = data2[~np.isinf(data2).any(axis=1)]\n",
    "    # print(data.shape)\n",
    "    # 预处理,wave,MSC,\n",
    "    # data_process=data\n",
    "\n",
    "\n",
    "    # data_process = preprocessSpectral.SG(data.to_numpy(),11,1)\n",
    "    # data_process = preprocessSpectral.MSC(data_process)\n",
    "    # data_process = preprocessSpectral.MSC(data_process)\n",
    "    # data_process = preprocessSpectral.D1(data.to_numpy())\n",
    "    data_process2 = preprocessSpectral.MSC(data2.to_numpy())\n",
    "\n",
    "    data_process2 = preprocessSpectral.SG(data_process2,11,1)\n",
    "\n",
    "    data_process2 = preprocessSpectral.SG(data_process2,17,1)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # data_process = preprocessSpectral.D1(data_process)\n",
    "    # data_process = signal.detrend(data_process)\n",
    "    \n",
    "    print(file)\n",
    "    print(data2.shape)\n",
    "    # 保存图像\n",
    "    # plt.plot(np.transpose(data_process)) \n",
    "    plt.savefig(sys.path[0]+ \"/hyperdata/data_preprocess/\" + filename.replace('xlsx', 'jpg'))\n",
    "    plt.clf()\n",
    "    # 加入数据\n",
    "    data2 = pd.DataFrame(data_process2)\n",
    "    # 标准差\n",
    "    sd[filename.split('_')[0]] = data2.std(axis=0)\n",
    "    # avgs[filename.split('.')[0]] = data.mean(axis=0)\n",
    "    data2.loc[:, 'label'] = label\n",
    "    dfs2.append(data2) \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas2 = pd.concat(dfs2,axis=0)\n",
    "datas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovr分类使用\n",
    "labels2 = datas2['label']\n",
    "datas2 = datas2.drop(columns='label')\n",
    "datalabels2 = pd.concat([datas2,labels2],axis=1)\n",
    "datalabels2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "# classes = ['宜糯红4号','泸州红1号','郎糯红19号']\n",
    "for i in range(1,7):\n",
    "    # traindata,testdata = train_test_split(datalabels[datalabels['label'] == i],test_size=0.2,random_state=0,shuffle=True)\n",
    "    traindata=datalabels[datalabels['label'] == i]\n",
    "    testdata=datalabels2[datalabels2['label'] == i]\n",
    "    train_data.append(traindata.drop(columns='label'))\n",
    "    train_label.append(traindata['label'])\n",
    "    test_data.append(testdata.drop(columns='label'))\n",
    "    test_label.append(testdata['label'])\n",
    "\n",
    "\n",
    "train_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "test_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "for i in range(0,len(train_data)):\n",
    "    train_pd = train_pd.append(pd.Series({'datapath':train_data[i],'label':train_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "for i in range(0,len(test_data)):\n",
    "    test_pd = test_pd.append(pd.Series({'datapath':test_data[i],'label':test_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "train_pd = shuffle(train_pd)\n",
    "test_pd = shuffle(test_pd)\n",
    "\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 手动指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 200\n",
    "startindexs = []\n",
    "for i in range(int(datas.shape[0]/samples)):\n",
    "    startindexs.append(i * samples)\n",
    "startindexs\n",
    "\n",
    "# 生成测试的索引\n",
    "test_data_index = []\n",
    "train_data_index = []\n",
    "for i in range(len(startindexs)):   \n",
    "    if((i-2)%4!=0):\n",
    "        train_data_index.append(i)       \n",
    "    else:\n",
    "        test_data_index.append(i)\n",
    "pass\n",
    "\n",
    "# 生成训练集与测试集数据\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "for i in train_data_index:\n",
    "    index = startindexs[i]\n",
    "    train_data.append(pd.DataFrame(datas.iloc[index:index+samples,:]))\n",
    "    train_label.append(pd.DataFrame(labels.iloc[index:index+samples,:]))\n",
    "for i in test_data_index:\n",
    "    index = startindexs[i]\n",
    "    test_data.append(pd.DataFrame(datas.iloc[index:index+samples,:]))\n",
    "    test_label.append(pd.DataFrame(labels.iloc[index:index+samples,:]))\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_all = datas\n",
    "test_label_all = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 自动分配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 带PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "pcas = []\n",
    "classes = ['1': 0,'2':1,'3':2]\n",
    "for i in range(3):\n",
    "    pca = PCA(n_components=20)\n",
    "\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels[classmap[i]] == 1],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.append(pd.DataFrame(pca.fit_transform(traindata.drop(columns=classes))))\n",
    "    train_label.append(traindata[classes])\n",
    "    test_data.append(pd.DataFrame(pca.transform(testdata.drop(columns=classes))))\n",
    "    test_label.append(testdata[classes])\n",
    "    \n",
    "    joblib.dump(pca,'hyperdata/outmodels/PCA/'+classes[i]+'.pkl')\n",
    "    pcas.append(pca)\n",
    "    # PCA降维\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    # \n",
    "\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 不带降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.1 one-hot类型的测试训练集分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "classes = ['021','bainong207','fanmai8hao','quanmai725','weilong169']\n",
    "for i in range(5):\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels[classmap[i]] == 1],test_size=0.2,random_state=0,shuffle=False)\n",
    "    train_data.append(traindata.drop(columns=classes))\n",
    "    train_label.append(traindata[classes])\n",
    "    test_data.append(testdata.drop(columns=classes))\n",
    "    test_label.append(testdata[classes])\n",
    "    \n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.2 OVR型测试训练集分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "# classes = ['宜糯红4号','泸州红1号','郎糯红19号']\n",
    "for i in range(1,7):\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels['label'] == i],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.append(traindata.drop(columns='label'))\n",
    "    train_label.append(traindata['label'])\n",
    "    test_data.append(testdata.drop(columns='label'))\n",
    "    test_label.append(testdata['label'])\n",
    "\n",
    "\n",
    "train_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "test_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "for i in range(0,len(train_data)):\n",
    "    train_pd = train_pd.append(pd.Series({'datapath':train_data[i],'label':train_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "for i in range(0,len(test_data)):\n",
    "    test_pd = test_pd.append(pd.Series({'datapath':test_data[i],'label':test_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "train_pd = shuffle(train_pd)\n",
    "test_pd = shuffle(test_pd)\n",
    "\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "# classes = ['021','bainong207','fanmai8hao','quanmai725','weilong169']\n",
    "for i in range(1,4):\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels['label'] == i],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.append(traindata.drop(columns='label'))\n",
    "    train_label.append(traindata['label'])\n",
    "    test_data.append(testdata.drop(columns='label'))\n",
    "    test_label.append(testdata['label'])\n",
    "    \n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.array(train_label)\n",
    "test_label=np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train_data)):\n",
    "    train_pd = train_pd.append(pd.Series({'datapath':train_data[i],'label':train_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "for i in range(0,len(test_data)):\n",
    "    test_pd = test_pd.append(pd.Series({'datapath':test_data[i],'label':test_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "\n",
    "train_pd = shuffle(train_pd)\n",
    "test_pd = shuffle(test_pd)\n",
    "train_data = np.array(train_pd['datapath'])\n",
    "train_label = np.array(pd.get_dummies(train_pd['label']))\n",
    "test_data = np.array(test_pd['datapath'])\n",
    "test_label = np.array(pd.get_dummies(test_pd['label']))\n",
    "# 训练模型\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 带波长选择的集合划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "# 波长选择\n",
    "waves = []\n",
    "# 手动根据STD指定\n",
    "waves = np.arange(0,25,1)\n",
    "waves = np.append(waves,np.arange(90, 110, 1),0)\n",
    "waves = np.append(waves,np.arange(135, 165, 1),0)\n",
    "waves = np.append(waves,np.arange(180, 190, 1),0)\n",
    "classes = ['021','bainong207','fanmai8hao','quanmai725','weilong169']\n",
    "for i in range(5):\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels[classmap[i]] == 1],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.append(pd.DataFrame(traindata.drop(columns=classes)[waves]))\n",
    "    train_label.append(traindata[classes])\n",
    "    test_data.append(pd.DataFrame(testdata.drop(columns=classes)[waves]))\n",
    "    test_label.append(testdata[classes])\n",
    "    pass\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 带波长选择的OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "# 波长选择\n",
    "waves = []\n",
    "# 手动根据STD指定\n",
    "waves = np.arange(0,5,1)\n",
    "waves = np.append(waves,np.arange(95, 105, 1),0)\n",
    "waves = np.append(waves,np.arange(145, 155, 1),0)\n",
    "waves = np.append(waves,np.arange(195, 200, 1),0)\n",
    "classes = ['021','bainong207','fanmai8hao','quanmai725','weilong169']\n",
    "for i in range(5):\n",
    "    traindata,testdata = train_test_split(datalabels[datalabels['label'] == i],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.append(pd.DataFrame(traindata.drop(columns='label')[waves]))\n",
    "    train_label.append(traindata['label'])\n",
    "    test_data.append(pd.DataFrame(testdata.drop(columns='label')[waves]))\n",
    "    test_label.append(testdata['label'])\n",
    "    pass\n",
    "train_data_all = pd.concat(train_data,axis=0)\n",
    "test_data_all = pd.concat(test_data,axis=0)\n",
    "train_label_all = pd.concat(train_label,axis=0)\n",
    "test_label_all = pd.concat(test_label,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newX[200:400,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(50,50))\n",
    "corr = datalabels.corr()\n",
    "ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap=\"RdYlGn\",annot=False)\n",
    "plt.title(\"Correlation between variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.建模"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 用之前的模型建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "# clf = DecisionTreeClassifier(random_state=0)\n",
    "# rfc = RandomForestClassifier(random_state=0)\n",
    "# rfc = rfc.fit(train_data_all,train_label_all)\n",
    "# 载入\n",
    "rfc = joblib.load('hyperdata/models/clf.pkl')\n",
    "y_pred = rfc.predict(test_data_all)\n",
    "print(classification_report(test_label_all, y_pred))\n",
    "print(accuracy_score(test_label_all, y_pred))\n",
    "rfc.score(test_data_all,test_label_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenSA.WaveSelect.WaveSelect import SpctrumFeatureSelcet\n",
    "from OpenSA.WaveSelect.Cars import CARS_Cloud\n",
    "wavefeaturelist = []\n",
    "\n",
    "for i in range(5):\n",
    "    idx = CARS_Cloud(datas.to_numpy(),labels.to_numpy()[:,i])\n",
    "    wavefeaturelist.append(idx)\n",
    "wavefeaturelist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(wavefeaturelist)\n",
    "s\n",
    "s.to_excel('hyperdata/models/carsfeature.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenSA.Preprocessing.Preprocessing import Preprocessing\n",
    "from OpenSA.WaveSelect.WaveSelect import SpctrumFeatureSelcet\n",
    "from OpenSA.Simcalculation.SimCa import Simcalculation\n",
    "from OpenSA.Clustering.Cluster import Cluster\n",
    "from OpenSA.Regression.Rgs import QuantitativeAnalysis\n",
    "from OpenSA.Classification.Cls import QualitativeAnalysis\n",
    "from OpenSA.DataLoad.DataLoad import SetSplit, LoadNirtest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #  忽略弹出的warnings信息\n",
    "def SpectralQualitativeAnalysis(data, label, ProcessMethods, FslecetedMethods, SetSplitMethods):\n",
    "    ProcesedData = Preprocessing(ProcessMethods, data)\n",
    "    FeatrueData, labels = SpctrumFeatureSelcet(FslecetedMethods, ProcesedData, label)\n",
    "    X_train, X_test, y_train, y_test = SetSplit(SetSplitMethods, FeatrueData, labels, test_size=0.2,randomseed=0)    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "waveselects = ['None','CARS','LARS','UVE','SPA','PCA']\n",
    "models = ['LDA','QDA','XGBOOST','ADABOOST','KNN','GBDT','ANN']\n",
    "for ws in waveselects:  \n",
    "    results = pd.DataFrame(columns=models)\n",
    "    for i in range(5): \n",
    "        X_train, X_test, y_train, y_test = SpectralQualitativeAnalysis(datas.to_numpy(),labels.to_numpy()[:,i],'None',ws,'random')\n",
    "        modelresults = pd.Series()   \n",
    "        for modelname in models:  \n",
    "            accuracy = QualitativeAnalysis(modelname, X_train, X_test, y_train, y_test)\n",
    "            modelresults[modelname] = accuracy\n",
    "            print(modelname + '-' + str(i) + \"-\" + ws)\n",
    "        pass\n",
    "        results = results.append(modelresults,ignore_index=True) \n",
    "    pass\n",
    "    results.to_excel(ws + '.xlsx')\n",
    "pass\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 单独测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenSA.Preprocessing.Preprocessing import Preprocessing\n",
    "from OpenSA.WaveSelect.WaveSelect import SpctrumFeatureSelcet\n",
    "from OpenSA.Simcalculation.SimCa import Simcalculation\n",
    "from OpenSA.Clustering.Cluster import Cluster\n",
    "from OpenSA.Regression.Rgs import QuantitativeAnalysis\n",
    "from OpenSA.Classification.Cls import QualitativeAnalysis\n",
    "from OpenSA.DataLoad.DataLoad import SetSplit, LoadNirtest\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #  忽略弹出的warnings信息\n",
    "\n",
    "# waveselects = ['None','CARS','LARS','UVE','SPA','PCA']\n",
    "models = ['PLS_DA','LDA','QDA','XGBOOST','ADABOOST','KNN','GBDT','RF']\n",
    "results = pd.DataFrame(columns=models)\n",
    "for i in range(5): \n",
    "    modelresults = pd.Series()   \n",
    "    for modelname in models:  \n",
    "        accuracy,tmodel = QualitativeAnalysis(modelname, train_data_all, test_data_all, train_label_all[classmap[i]], test_label_all[classmap[i]]) \n",
    "        modelresults[modelname] = accuracy\n",
    "        print(modelname + '-' + str(i))\n",
    "        joblib.dump(tmodel, 'hyperdata/outmodels/'+modelname+'-'+str(classmap[i])+'-'+'20221026'+'.pkl')\n",
    "    pass\n",
    "    results = results.append(modelresults,ignore_index=True) \n",
    "pass\n",
    "results.to_excel('NONE.xlsx')\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QualitativeAnalysis('RF', train_data_all, test_data_all, train_label_all, test_label_all )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 OVR多分类器测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #  忽略弹出的warnings信息\n",
    "\n",
    "# clf = OneVsOneClassifier(LinearSVC(),n_jobs=-1)\n",
    "models = [\n",
    "    # QDA(),\n",
    "    # LDA(),\n",
    "    # RandomForestClassifier(),\n",
    "    # DecisionTreeClassifier(),\n",
    "    # ExtraTreeClassifier(),\n",
    "    GradientBoostingClassifier(),  \n",
    "    # ExtraTreesClassifier(),\n",
    "    # # GaussianProcessClassifier(multi_class = 'one_vs_rest'),\n",
    "    # LogisticRegression(multi_class='ovr'),\n",
    "    # LogisticRegressionCV(multi_class='ovr'),\n",
    "    \n",
    "]\n",
    "'''查看在测试集的表现'''\n",
    "# result = pd.DataFrame(columns=['model','acc','recall','f1_score'])\n",
    "# for model in models:\n",
    "#     clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "#     clf.fit(train_data_all,train_label_all)\n",
    "#     y_pred = clf.predict(test_data_all)\n",
    "#     print(model.__class__)\n",
    "#     joblib.dump(clf, 'hyperdata/outmodels/OVR-models/'+model.__class__.__name__+'.pkl')\n",
    "#     with open('result.txt', 'a') as f:  # 设置文件对象\n",
    "#         print(model.__class__,file=f)    \n",
    "#         print(classification_report(test_label_all,y_pred),file=f)\n",
    "        \n",
    "#     result = result.append(pd.Series({\n",
    "#         'model' : model.__class__,\n",
    "#         'acc':accuracy_score(test_label_all,y_pred),\n",
    "#         'recall':recall_score(test_label_all, y_pred,average='macro'),\n",
    "#         'f1_score':f1_score(test_label_all, y_pred,average='macro'),\n",
    "#         }),ignore_index=True)\n",
    "    \n",
    "'''查看在训练集的表现'''\n",
    "result = pd.DataFrame(columns=['model','acc','recall','f1_score'])\n",
    "for model in models:\n",
    "    clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "    clf.fit(train_data_all,train_label_all)\n",
    "    y_pred = clf.predict(train_data_all)\n",
    "    print(model.__class__)\n",
    "    joblib.dump(clf, 'hyperdata/outmodels/OVR-models/'+model.__class__.__name__+'.pkl')\n",
    "    with open('result.txt', 'a') as f:  # 设置文件对象\n",
    "        print(model.__class__,file=f)    \n",
    "        print(classification_report(train_label_all,y_pred),file=f)\n",
    "        \n",
    "    result = result.append(pd.Series({\n",
    "        'model' : model.__class__,\n",
    "        'acc':accuracy_score(train_label_all,y_pred),\n",
    "        'recall':recall_score(train_label_all, y_pred,average='macro'),\n",
    "        'f1_score':f1_score(train_label_all, y_pred,average='macro'),\n",
    "        }),ignore_index=True)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_num = len(model.get_params())\n",
    "\n",
    "# 输出参数数量\n",
    "print(\"Model的参数量为：\", params_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集结果画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data = confusion_matrix(test_label_all, y_pred)\n",
    "names=['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(names), index = np.unique(names))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, cmap=\"Reds\", annot=True,annot_kws={\"size\": 16}, fmt='d')\n",
    "\n",
    "plt.savefig('cmap.png', dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集结果画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_label_all, y_pred))\n",
    "accuracy_score(test_label_all,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_label_all, y_pred))\n",
    "accuracy_score(train_label_all,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data = confusion_matrix(train_label_all, y_pred)\n",
    "names=['Lu1','Lang19','Yi4','Jiu9','Ai','Kang']\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(names), index = np.unique(names))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, cmap=\"Reds\", annot=True,annot_kws={\"size\": 16}, fmt='d')\n",
    "\n",
    "plt.savefig('cmap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 OVO多分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #  忽略弹出的warnings信息\n",
    "result = pd.DataFrame(columns=['model','acc'])\n",
    "# clf = OneVsOneClassifier(LinearSVC(),n_jobs=-1)\n",
    "models = [QDA,LDA,RandomForestClassifier,LinearSVC,LinearSVR,NuSVC,NuSVR,SVC,SVR,DecisionTreeClassifier,ExtraTreeClassifier,\n",
    "AdaBoostClassifier,GradientBoostingClassifier]\n",
    "for model in models:\n",
    "    clf = OneVsOneClassifier(model(),n_jobs=-1)\n",
    "    clf.fit(train_data_all,train_label_all)\n",
    "    y_pred = clf.predict(test_data_all)\n",
    "    result = result.append(pd.Series({'model' : model.__name__,'acc':accuracy_score(test_label_all,y_pred)}),ignore_index=True)\n",
    "\n",
    "# \n",
    "# rfc = RandomForestClassifier(random_state=123)\n",
    "# rfc = rfc.fit(train_data_all,train_label_all)\n",
    "# y_pred = rfc.predict(test_data_all)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_label_all, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "for k,v in labelDict.items():\n",
    "    rfc = QDA()\n",
    "    # CARS特征\n",
    "    carsfeatures = pd.read_excel('hyperdata/models/carsfeature.xlsx')\n",
    "    cf = carsfeatures.iloc[v,:].tolist()\n",
    "    new_list=[]\n",
    "    for elem in cf:\n",
    "        if not np.isnan(elem):\n",
    "            new_list.append(elem)\n",
    "    rfc = rfc.fit(datas.iloc[:,new_list],labels.iloc[:,v])\n",
    "    joblib.dump(rfc, 'hyperdata/models/QDA4000-v1.0-'+str(k)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 随机分组交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier as RF                       # 随机森林\n",
    "def kFold_cv(X, y, classifier, **kwargs):\n",
    "    \"\"\"\n",
    "    :param X: 特征\n",
    "    :param y: 目标变量\n",
    "    :param classifier: 分类器\n",
    "    :param **kwargs: 参数\n",
    "    :return: 预测结果\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=5, shuffle=True) \n",
    "    y_pred = np.zeros(shape=(len(y),5))         # 初始化y_pred数组\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):  \n",
    "        X_train = X[train_index]    \n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]      # 划分数据集\n",
    "        clf = classifier(**kwargs)    \n",
    "        clf.fit(X_train, y_train)     # 模型训练\n",
    "        y_pred[test_index,:] = clf.predict(X_test)  # 模型预测\n",
    "    \n",
    "    return y_pred \n",
    "\n",
    "y_PRED = kFold_cv(datas.to_numpy(), labels.to_numpy(), RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def metrics_data(y,ypred):\n",
    "\n",
    "    accuracy = accuracy_score(y, ypred)\n",
    "    precision = precision_score(y, ypred,average='micro')\n",
    "    recall = recall_score(y, ypred,average='micro')\n",
    "    f1 = f1_score(y, ypred,average='micro')\n",
    "    print('accuracy:{}'.format(accuracy))\n",
    "    print('precision:{}'.format(precision))\n",
    "    print('recall:{}'.format(recall))\n",
    "    print('f1-score:{}'.format(f1))\n",
    "    metrics = pd.DataFrame(columns=['accuracy','precision','recall','f1'])\n",
    "    metrics = metrics.append(pd.Series({\"accuracy\":accuracy,\"precision\":precision,'recall':recall,'f1':f1}),ignore_index=True)\n",
    "    print(classification_report(y, ypred))\n",
    "    return metrics\n",
    "metrics_data(test_label_all,ypred=y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#保存Model(注:save文件夹要预先建立，否则会报错)\n",
    "joblib.dump(rfc, 'hyperdata/models/clf.pkl')\n",
    "\n",
    "#读取Model\n",
    "clf3 = joblib.load('hyperdata/models/clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "# clf = DecisionTreeClassifier(random_state=0)\n",
    "# rfc = RandomForestClassifier(random_state=0)\n",
    "# rfc = rfc.fit(train_data_all,train_label_all)\n",
    "# 载入\n",
    "models = ['PLS_DA','LDA','QDA','XGBOOST','ADABOOST','KNN','GBDT','RF']\n",
    "def loadMyModels(type='all',modelname='XGBOOST'):\n",
    "    loadmodels = []\n",
    "    pcamodel = []\n",
    "    if type == 'all':\n",
    "        for k,v in labelDict.items():\n",
    "            loadmodels.append(joblib.load('hyperdata/outmodels/'+modelname+'-'+k+'-20221026.pkl'))\n",
    "            pcamodel.append(joblib.load('hyperdata/outmodels/PCA/'+k+'.pkl'))\n",
    "        pass\n",
    "    else:\n",
    "        loadmodels.append(joblib.load('hyperdata/outmodels/'+modelname+'-'+type+'-20221026.pkl'))\n",
    "        pcamodel.append(joblib.load('hyperdata/outmodels/PCA/'+type+'.pkl'))\n",
    "\n",
    "    return loadmodels,pcamodel\n",
    "\n",
    "def useMyModels(loadmodels,x_test,y_test):\n",
    "    loadModelAccuracys = []\n",
    "    predResuls = pd.DataFrame()\n",
    "    # CARS特征\n",
    "    carsfeatures = pd.read_excel('hyperdata/models/carsfeature.xlsx')\n",
    "    count = 0\n",
    "    for m in loadmodels:\n",
    "        cf = carsfeatures.iloc[count,:].tolist()\n",
    "        new_list=[]\n",
    "        for elem in cf:\n",
    "            if not np.isnan(elem):\n",
    "                new_list.append(elem)\n",
    "        \n",
    "        re = m.predict(x_test.iloc[:,new_list])\n",
    "        predResuls[str(count)] = re\n",
    "        count = count+1\n",
    "    return predResuls\n",
    "\n",
    "def useMyModels2(loadmodels,x_test,y_test,name):\n",
    "    predResuls = pd.DataFrame()\n",
    "    probResuls = pd.DataFrame()\n",
    "    loadModelAccuracys = []\n",
    "    count = 0\n",
    "    for m in loadmodels:\n",
    "        re = m.predict(x_test)\n",
    "        # p = m.predict_proba(x_test)\n",
    "        if name == 'PLS_DA':\n",
    "            re = np.array([np.argmax(i) for i in re])\n",
    "        # probResuls[classmap[count]] = p\n",
    "        count = count + 1\n",
    "    return re,probResuls\n",
    "\n",
    "def useMyModels3(loadmodels,x_test,y_test,pcamodels,type,name):\n",
    "    predResuls = pd.DataFrame()\n",
    "    probResuls = pd.DataFrame()\n",
    "    loadModelAccuracys = []\n",
    "    for m in loadmodels:\n",
    "        newx = pcamodels[0].transform(x_test)\n",
    "        \n",
    "        re = m.predict(newx)\n",
    "        if name == 'PLS_DA':\n",
    "             re = np.array([np.argmax(i) for i in re])\n",
    "    return re,probResuls\n",
    "# \n",
    "results = pd.DataFrame(columns=models)\n",
    "for l in classes:\n",
    "    modelresults = pd.Series() \n",
    "    for m in models:\n",
    "        loadmodels,pcamodel = loadMyModels(l,m)\n",
    "        pred,prob = useMyModels2(loadmodels=loadmodels,x_test=datas,y_test=labels,name=m)\n",
    "        # PCA分类用\n",
    "        # pred,prob = useMyModels3(loadmodels=loadmodels,x_test=datas,y_test=labels,pcamodels=pcamodel,type=l,name=m)\n",
    "        modelresults[m] = precision_score(pred, labels[l])\n",
    "    pass\n",
    "    results = results.append(modelresults,ignore_index=True) \n",
    "pass\n",
    "results.to_excel('NONE-预测集表现.xlsx')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 OVR模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #  忽略弹出的warnings信息\n",
    "\n",
    "\n",
    "def loadMyModels(modelname='XGBOOST'):\n",
    "    return  joblib.load('hyperdata/outmodels/OVR-models/'+modelname+'.pkl')\n",
    "\n",
    "def useMyModels(loadmodels,x_test,y_test):\n",
    "    loadModelAccuracys = []\n",
    "    re = loadmodels.predict(x_test.iloc[:,waves])\n",
    "    # re = loadmodels.predict(x_test.iloc[:,:])\n",
    "    return re\n",
    "\n",
    "models = [\n",
    "    # QDA(),\n",
    "    LDA(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    # GaussianProcessClassifier(multi_class = 'one_vs_rest'),\n",
    "    # LogisticRegression(multi_class='ovr'),\n",
    "    # LogisticRegressionCV(multi_class='ovr'),\n",
    "    ExtraTreesClassifier()\n",
    "]\n",
    "\n",
    "modelclsnames = []\n",
    "for mod in models:\n",
    "    modelclsnames.append(mod.__class__.__name__)\n",
    "\n",
    "results = pd.DataFrame(columns=modelclsnames)\n",
    "modelresults = pd.Series() \n",
    "for m in models:\n",
    "    loadmodel = loadMyModels(m.__class__.__name__)\n",
    "    pred= useMyModels(loadmodels=loadmodel,x_test=datas,y_test=labels)\n",
    "    modelresults[m.__class__.__name__] = accuracy_score(pred, labels)\n",
    "    with open('result_prd.txt', 'a') as f:  # 设置文件对象\n",
    "        print(model.__class__,file=f)    \n",
    "        print(classification_report(labels,pred),file=f)\n",
    "pass\n",
    "results = results.append(modelresults,ignore_index=True) \n",
    "\n",
    "results.to_excel('NONE-预测集表现.xlsx')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmodelPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.深度学习分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
