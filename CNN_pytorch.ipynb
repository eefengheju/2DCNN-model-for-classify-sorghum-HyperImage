{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard import program\n",
    "import torch\n",
    "writer = SummaryWriter(log_dir='./log/train/') #日志存储的路径\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分训练集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "labelDict = {'1': 0,'2':1,'3':2,'G11': 3,'G12':4,'G13':5}\n",
    "folders = glob.glob('indata/npy224/*')\n",
    "# 数据\n",
    "datas = []\n",
    "for i in range(6):\n",
    "    datas.append([])\n",
    "    pass\n",
    "\n",
    "# 载入数据\n",
    "for folder in folders:      \n",
    "    folder = folder.replace('\\\\', '/')\n",
    "    label = labelDict[folder.split('/')[-1].split('_')[0]]\n",
    "    # 数据提取\n",
    "    npys = glob.glob(folder + '/*.npy')\n",
    "    for npy in npys:\n",
    "        npy = npy.replace('\\\\', '/')\n",
    "        # data_npy = np.load(file=npy)\n",
    "        datas[label].append(npy)    \n",
    "        pass\n",
    "    pass\n",
    "datas\n",
    "datanp = np.array(datas)\n",
    "datanp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "# datanp=shuffle(datanp) \n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "for label in range(6):\n",
    "    traindata,testdata = train_test_split(datanp[label],test_size=0.2,random_state=0,shuffle=True)\n",
    "    train_data.extend(traindata)\n",
    "    test_data.extend(testdata)\n",
    "    train_shape = np.array(traindata).shape\n",
    "    test_shape = np.array(testdata).shape\n",
    "    train_label.extend(np.full(train_shape,label))\n",
    "    test_label.extend(np.full(test_shape,label))\n",
    "     \n",
    "# train_data = np.array(train_data)\n",
    "# train_label = np.array(pd.get_dummies(train_label))\n",
    "# test_data = np.array(test_data)\n",
    "# test_label = np.array(pd.get_dummies(test_label))\n",
    "\n",
    "train_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "test_pd = pd.DataFrame(columns=['datapath','label'])\n",
    "for i in range(0,len(train_data)):\n",
    "    train_pd = train_pd.append(pd.Series({'datapath':train_data[i],'label':train_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "for i in range(0,len(test_data)):\n",
    "    test_pd = test_pd.append(pd.Series({'datapath':test_data[i],'label':test_label[i]}),ignore_index=True)\n",
    "    pass\n",
    "train_pd = shuffle(train_pd)\n",
    "test_pd = shuffle(test_pd)\n",
    "# train_pd = pd.concat([pd.DataFrame(train_data),pd.DataFrame(train_label)],axis=1)\n",
    "# test_pd = pd.concat([pd.DataFrame(test_data),pd.DataFrame(test_label)],axis=1)\n",
    "train_data = np.array(train_pd['datapath'])\n",
    "train_label = np.array(pd.get_dummies(train_pd['label']))\n",
    "test_data = np.array(test_pd['datapath'])\n",
    "test_label = np.array(pd.get_dummies(test_pd['label']))\n",
    "# test_label = np.array(test_label)\n",
    "# 训练模型\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.savetxt('train2.txt', train_data,fmt='%s', delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用之前的划分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "import torch.nn\n",
    "class LoadData(dataset.Dataset):\n",
    "\n",
    "    def __init__(self,data_paths,is_train = True):\n",
    "        super(LoadData, self).__init__()\n",
    "        self.data_paths = data_paths\n",
    "        self.is_train = is_train\n",
    "        self.label_Dict = {'1': 0,'2':1,'3':2,'G11': 3,'G12':4,'G13':5}\n",
    "        # 数据预处理\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        data_path = self.data_paths[index]\n",
    "        data_name_arr = data_path.split('\\\\')\n",
    "        print(data_name_arr)\n",
    "        key = data_name_arr[-2].split('_')[0]\n",
    "        label = self.label_Dict[key]\n",
    "        data = np.load(data_path)\n",
    "        data[np.isinf(data)] = 0\n",
    "        data[np.isnan(data)] = 0\n",
    "        \n",
    "        # print(data_path)\n",
    "        # print(data.shape)\n",
    "        # 修改维度,1DCNN注释此行，2D3D不注释\n",
    "        tensor = data.transpose(2,0,1)\n",
    "        # 3DCNN增加下面行代码，2DCNN不加\n",
    "        # tensor = np.expand_dims(data, axis=0)\n",
    "        return tensor,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import dataloader\n",
    "from torch.utils.data import dataset\n",
    "f1=open('train2.txt', encoding='utf-8')\n",
    "f2=open('test2.txt', encoding='utf-8')\n",
    "path1=[]\n",
    "path2=[]\n",
    "for line in f1:\n",
    "    path1.append(line.strip())\n",
    "for line in f2:\n",
    "    path2.append(line.strip())\n",
    "# 读取train test进入list\n",
    "train_dataset = LoadData(data_paths=path1,is_train=True)\n",
    "test_dataset = LoadData(data_paths=path2,is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet2D(nn.Module):\n",
    "    def __init__(self, width=30, height=30, channel=224):\n",
    "        super(ConvNet2D, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=224, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=64)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(in_features=64*height*width//4, out_features=224)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=[224])\n",
    "        self.fc2 = nn.Linear(in_features=224, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        aver = torch.mean(x, dim=1)\n",
    "        aver = torch.mean(aver, dim=0)\n",
    "        aver = torch.mean(aver, dim=1)\n",
    "        x=x.reshape(1,30,30,224)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64*self.width*self.height//4)\n",
    "        x = F.glorot_uniform(self.fc1(x))\n",
    "        x = self.layer_norm(x + aver)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the Model2D class\n",
    "model = ConvNet2D().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# model = torch.load('default+pre_norm+drop=0.5.pt')\n",
    "# model = model.to(device)\n",
    "import glob\n",
    "from torch.utils.data import dataloader\n",
    "from torch.utils.data import dataset\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer,'max',0.7,15,eps=1e-6)\n",
    "batch_size = 16\n",
    "train_loader = dataloader.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "test_loader = dataloader.DataLoader(\n",
    "dataset=test_dataset,\n",
    "batch_size=batch_size,\n",
    "shuffle=True\n",
    "    )\n",
    "epoch = 200\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "for t in range(epoch):\n",
    "    right = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    \n",
    "    loop = tqdm(enumerate(train_loader), total =len(train_loader))\n",
    "    for step, (x, y) in loop:\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        train_total += y.size(0)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y) # 计算损失函数\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad() # 梯度置零，因为反向传播过程中梯度会累加上一次循环的梯度\n",
    "        loss.backward() # loss反向传播\n",
    "        optimizer.step() # 反向传播后参数更新\n",
    "      \n",
    "        running_loss += loss.item()\n",
    "        # 可视化graph\n",
    "        if t == 0:\n",
    "            writer.add_graph(model,x)\n",
    "        # 可视化图像\n",
    "        # img_grid = torchvision.utils.make_grid(x)\n",
    "        # writer.add_image('hyperspectral_wheat_seed',img_grid)\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        # 累加识别正确的样本数\n",
    "        right += (predicted == y).sum()\n",
    "        acc = float(right)/float(batch_size*step+len(x))\n",
    "        #更新信息\n",
    "        loop.set_description(f'Epoch [{t}/{epoch}]')\n",
    "        loop.set_postfix(train_loss=loss.item(), train_acc=acc,lr=optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "   \n",
    "        # loop.set_postfix(loss=running_loss/(step+1), acc=acc)\n",
    "        if step+1 == len(train_loader):\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss,\n",
    "                            epoch * len(train_loader) + step)\n",
    "            writer.add_scalar('training acc',\n",
    "                            acc,\n",
    "                            epoch * len(train_loader) + step)\n",
    "        # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "        # random mini-batch\n",
    "        # writer.add_figure('predictions vs. actuals',\n",
    "        #                 plot_classes_preds(net, inputs, labels),\n",
    "        #                 global_step=epoch * len(train_loader) + i)\n",
    "        running_loss = 0.0\n",
    "       # 测试模式\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_sum_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        loop2 = tqdm(test_loader, desc='Test')\n",
    "        for x, y in loop2:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            y_ = torch.argmax(y_pred, dim=1)\n",
    "            test_correct += (y_ == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "            test_sum_loss += loss.item()\n",
    "            test_running_loss = test_sum_loss / test_total\n",
    "            test_running_acc = test_correct / test_total\n",
    "            \n",
    "            # 更新测试信息\n",
    "            loop2.set_postfix(loss = test_running_loss, acc = test_running_acc)\n",
    "           \n",
    "    test_epoch_loss = test_sum_loss / test_total\n",
    "    test_epoch_acc = test_correct / test_total\n",
    "    scheduler.step(test_running_acc)\n",
    "print('Finished Training')\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
